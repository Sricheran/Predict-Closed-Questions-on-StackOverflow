DOC CODE OR METHOLODIGY PREP FLOW:

-Showing the reason for not using 33L dataset and going for Sample Dataset

-Shows tokens without stop words and getting top words based on Count of top 10 =>
Vocabulary Size: 411272
Most Common Words: [('1', 116406), ('0', 100576), ('2', 64041), ('using', 59058), ('code', 57381), ('new', 55681), ('like', 53264), ('c', 51703), ('java', 48905), ('get', 47378), ('class', 46101), ('com', 45630), ('file', 45532), ('http', 44910), ('use', 43750), ('want', 43719), ('data', 41971), ('id', 41814), ('name', 41737), ('would', 41692)]

-Bag of Words, Its Matrix and showing Sparsity => Sparsity: 99.61%

-TF IDF, Its Matrix and showing Sparsity => Sparsity: 99.61%

-TFIDF AND BAG OF WORDS MULTICLASS CLASSIFICATION Using 10000 Features => 
TF-IDF Logistic Regression Accuracy: 0.6334
BoW Logistic Regression Accuracy: 0.5915

-TFIDF AND BAG OF WORDS MULTICLASS CLASSIFICATION Using All Features =>
TF-IDF Logistic Regression Accuracy: 0.6327
BoW Logistic Regression Accuracy: 0.5889 


######### Instead of MULTICLASS, we decided to go for BINARY CLASSIFICATION ############


-TFIDF AND BAG OF WORDS BINARY CLASSIFICATION Using 10000 Features
TF-IDF Logistic Regression Accuracy: 0.7400
BoW Logistic Regression Accuracy: 0.7203

-TFIDF AND BAG OF WORDS BINARY CLASSIFICATION Using ALL Features
TF-IDF Logistic Regression Accuracy: 0.7351
BoW Logistic Regression Accuracy: 0.7148

-WORD2VEC CBOW AND SKIPGRAM BINARY CLASSIFICATION USING NAIVE BAYES
Naive Bayes with CBOW Accuracy: 0.6808412047763323
Naive Bayes with Skip-gram Accuracy: 0.6806629834254143

-WORD2VEC CBOW AND SKIPGRAM BINARY CLASSIFICATION USING LOGISTIC REGRESSION
Logistic Regression with CBOW Accuracy: 0.7159151666369631
Logistic Regression with Skip-gram Accuracy: 0.7217251826768847

-WORD2VEC CBOW AND SKIPGRAM MULTICLASS CLASSIFICATION USING LOGISTIC REGRESSION
(CBOW)Accuracy: 0.6168240955266441
(Skip-gram)Accuracy: 0.6197825699518802

##### We tried Manual check Misclassification and gave custom inputs to check on outputs #####
####Instead of word embedding we went for sentence embeddings###

SBERT:
TITLE+BODYMARKDOWN = 0.7552307966494386
TITLE+BODYMARKDOWN (with 4 classes) = 0.6507574407414008
TITLE ONLY= 0.7134200677241134
BODYMARKDOWN ONLY = 0.7493494920691499
CONCATINATE { TITLE + BODYMARKDOWN(WITH CODE) + TAGS + CODE} = 0.7698
CONCATINATE { TITLE + BODYMARKDOWN(WITHOUT CODE) + TAGS + CODE} = 0.7641
CONCATINATE { TITLE + BODYMARKDOWN(WITHOUT CODE) + CODE} = 0.7593
CONCATINATE { TITLE + BODYMARKDOWN(WITH CODE) + CODE} = 0.7645
CONCATINATE { TITLE + BODYMARKDOWN(WITH CODE)} = 0.7614


CONCATINATE { TITLE + BODYMARKDOWN(WITH CODE) + TAGS + CODE} => (USING SVM LINEAR KERNEL) = 0.7706
CONCATINATE { TITLE + BODYMARKDOWN(WITH CODE) + TAGS + CODE} => (MUTLICLASS CLASSIFICATION AND OUTPUT AS BINARY CLASSIFICATION) =  0.7642
CONCATINATE { TITLE + BODYMARKDOWN(WITH CODE) + {TAG1+....+TAG5}(Word2Vec) + CODE} = 0.7667
CONCATINATE { TITLE + BODYMARKDOWN(WITH CODE) + {TAG1+....+TAG5}(Word2Vec) with permutations(mean) + CODE} =  0.7662
CONCATINATE { SUMMARY + CODE} = 0.6970
CONCATINATE { SUMMARY + TAGS + CODE} = 0.7158
CONCATINATE { SUMMARY + TITLE + BODYMARKDOWN(WITH CODE) + CODE} = 0.7654
CONCATINATE { SUMMARY + TITLE + BODYMARKDOWN(WITH CODE) + TAGS + CODE} = 0.7689


### Instead of Logistic Regression, shifted to Neural Network ###


GENERAL NN WITH DROPOUT and HID=512 => Test Accuracy: 77.67%
GENERAL NN WITH DROPOUT and HID=128 => Test Accuracy: 77.16%
GENERAL NN WITH DROPOUT and HID=64 => Test Accuracy: 77.34%
Residual with hid=128 => Test Accuracy: 77.55%
Residual with HID=64 => Test Accuracy: 77.53%
DenseNet with HID=128 => Test Accuracy: 77.51%
DenseNet with HID=64 => Test Accuracy: 77.48%

SBERT CONCATINATE {TITLE + BODYMARKDOWN(WITH CODE) + TAGS + CODE} with MULTICLASS CLASSIFIER AS NN => Test Accuracy: 67.23%


##### BERT ######

BERT CONCATINATE { TITLE + BODYMARKDOWN(WITH CODE) + TAGS + CODE} => 76.82%
BERT CONCATINATE { TITLE + BODYMARKDOWN(WITH CODE) + TAGS + CODE} Multiclass classification => 67.91%
BERT CONCATINATE { TITLE + BODYMARKDOWN(WITH CODE) + TAGS + CODE} CLASIFIER AS NN => Test Accuracy: 0.7470


##### Later took two models one will predict 'open' or 'close' , if its 'open' fine. If its 'close', then given to other model which only predicts classes of closed #####

CASCADING PREDICTIONS USING TWO LOGIESTIC REGRESSION MODELS => Final Accuracy: 66.60%
CASCADING PREDICTIONS USING TWO FINE TUNE BERT MODELS => 53.00%

##### FINE TUNE BERT #####
BERT BINARY CLASSFICATION (TRAINED ON 3 EPOCHS) => 79.58%
BERT MULTICLASS CLASSFICATION (TRAINED ON 3 EPOCHS) => 69.00%





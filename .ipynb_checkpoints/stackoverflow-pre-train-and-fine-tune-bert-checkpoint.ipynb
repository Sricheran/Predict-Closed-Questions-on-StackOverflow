{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-06T02:19:45.658625Z",
     "iopub.status.busy": "2025-04-06T02:19:45.658274Z",
     "iopub.status.idle": "2025-04-06T02:19:51.016172Z",
     "shell.execute_reply": "2025-04-06T02:19:51.015515Z",
     "shell.execute_reply.started": "2025-04-06T02:19:45.658597Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"/kaggle/input/train-smaple-with-code/train-sample_with_code.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:34:27.691898Z",
     "iopub.status.busy": "2025-04-05T06:34:27.691671Z",
     "iopub.status.idle": "2025-04-05T12:25:13.208022Z",
     "shell.execute_reply": "2025-04-05T12:25:13.207292Z",
     "shell.execute_reply.started": "2025-04-05T06:34:27.691878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b63ed8e4674a06abc46d17cbb0fbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143e4a98ecb54a8584ab73fee7b6d8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c60c00087774262b011123e05fbebaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866d6a5ead2b4b4f994b4626993503a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c22d97aadd4cfb8197f976d1f29ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.4756, Val Loss = 0.4601, Val Acc = 0.7836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.3918, Val Loss = 0.4534, Val Acc = 0.7958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.2962, Val Loss = 0.5181, Val Acc = 0.7910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned BERT Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.79     14117\n",
      "           1       0.79      0.78      0.79     13938\n",
      "\n",
      "    accuracy                           0.79     28055\n",
      "   macro avg       0.79      0.79      0.79     28055\n",
      "weighted avg       0.79      0.79      0.79     28055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Preprocess DataFrame\n",
    "df[['Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5']] = df[['Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5']].fillna('')\n",
    "df['Tags_combined'] = df[['Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5']].apply(lambda x: ' '.join(x), axis=1)\n",
    "df = df[['Title', 'BodyMarkdown', 'Tags_combined', 'OpenStatus']].dropna()\n",
    "df['OpenStatus'] = df['OpenStatus'].map(lambda x: 1 if x == \"open\" else 0)\n",
    "df[\"text\"] = df.apply(lambda row: f\"Title: {row['Title']} Body: {row['BodyMarkdown']} Tags: {row['Tags_combined']}\", axis=1)\n",
    "\n",
    "# Train-test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"text\"].tolist(), df[\"OpenStatus\"].tolist(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Custom Dataset\n",
    "class SOFDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = SOFDataset(train_encodings, train_labels)\n",
    "val_dataset = SOFDataset(val_encodings, val_labels)\n",
    "\n",
    "# DataLoaders (batch size increased for multi-GPU use)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Load Pretrained BERT Model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)  # Wrap for multi-GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer, Scheduler, Loss\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4)\n",
    "num_training_steps = len(train_loader) * 3\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 3\n",
    "counter = 0\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = criterion(outputs.logits, batch[\"labels\"])\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_true.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = accuracy_score(val_true, val_preds)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "    # Early Stopping Checkpoint\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), \"best_fine_tuned_bert.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered! Loading best model...\")\n",
    "            model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                model = nn.DataParallel(model)\n",
    "            model.load_state_dict(torch.load(\"/kaggle/working/best_fine_tuned_bert.pth\"))\n",
    "            model = model.to(device)\n",
    "            break\n",
    "\n",
    "# Final Evaluation\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "# Print Metrics\n",
    "print(\"Fine-Tuned BERT Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:27:40.220150Z",
     "iopub.status.busy": "2025-04-05T12:27:40.219855Z",
     "iopub.status.idle": "2025-04-05T12:27:40.723583Z",
     "shell.execute_reply": "2025-04-05T12:27:40.722896Z",
     "shell.execute_reply.started": "2025-04-05T12:27:40.220129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples saved to 'misclassified_finetuned_bert.csv'\n"
     ]
    }
   ],
   "source": [
    "# Convert validation texts and original label lists back to a DataFrame\n",
    "val_df = pd.DataFrame({\n",
    "    \"text\": val_texts,\n",
    "    \"True_Label\": y_true,\n",
    "    \"Predicted_Label\": y_pred\n",
    "})\n",
    "\n",
    "# Re-split text back into Title, BodyMarkdown, Tags_combined for CSV output\n",
    "val_df[[\"Title\", \"BodyMarkdown\", \"Tags_combined\"]] = val_df[\"text\"].str.extract(\n",
    "    r\"Title: (.*?) Body: (.*?) Tags: (.*)\", expand=True)\n",
    "\n",
    "# Filter misclassifications\n",
    "misclassified_df = val_df[val_df[\"True_Label\"] != val_df[\"Predicted_Label\"]]\n",
    "\n",
    "# Save misclassifications to CSV\n",
    "misclassified_df.to_csv(\"misclassified_finetuned_bert.csv\", index=False)\n",
    "print(\"Misclassified examples saved to 'misclassified_finetuned_bert.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:37:42.182842Z",
     "iopub.status.busy": "2025-04-05T12:37:42.182527Z",
     "iopub.status.idle": "2025-04-05T12:37:46.621122Z",
     "shell.execute_reply": "2025-04-05T12:37:46.620402Z",
     "shell.execute_reply.started": "2025-04-05T12:37:42.182821Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split the 'text' column into Title, BodyMarkdown, Tags_combined safely\n",
    "def extract_parts(text):\n",
    "    match = re.match(r\"Title: (.*?) Body: (.*?) Tags: (.*)\", text, flags=re.DOTALL)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    else:\n",
    "        return (\"\", \"\", \"\")\n",
    "\n",
    "# Apply extraction\n",
    "val_df[[\"Title\", \"BodyMarkdown\", \"Tags_combined\"]] = val_df[\"text\"].apply(lambda x: pd.Series(extract_parts(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:38:36.754958Z",
     "iopub.status.busy": "2025-04-05T12:38:36.754643Z",
     "iopub.status.idle": "2025-04-05T12:38:37.008479Z",
     "shell.execute_reply": "2025-04-05T12:38:37.007537Z",
     "shell.execute_reply.started": "2025-04-05T12:38:36.754935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples saved to 'misclassified_finetuned_bert.csv'\n"
     ]
    }
   ],
   "source": [
    "misclassified_df = val_df[val_df[\"True_Label\"] != val_df[\"Predicted_Label\"]]\n",
    "\n",
    "# Save misclassifications to CSV\n",
    "misclassified_df.to_csv(\"misclassified_finetuned_bert.csv\", index=False)\n",
    "print(\"Misclassified examples saved to 'misclassified_finetuned_bert.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:39:40.401919Z",
     "iopub.status.busy": "2025-04-05T12:39:40.401580Z",
     "iopub.status.idle": "2025-04-05T12:39:40.407339Z",
     "shell.execute_reply": "2025-04-05T12:39:40.406502Z",
     "shell.execute_reply.started": "2025-04-05T12:39:40.401890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5863"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(misclassified_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:50:53.457765Z",
     "iopub.status.busy": "2025-04-05T12:50:53.457367Z",
     "iopub.status.idle": "2025-04-05T12:51:21.964795Z",
     "shell.execute_reply": "2025-04-05T12:51:21.963851Z",
     "shell.execute_reply.started": "2025-04-05T12:50:53.457737Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-b1faf074dbe1>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"/kaggle/working/best_fine_tuned_bert.pth\", map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the Title:  stop ajax function in midway when other element is clicked\n",
      "Enter the BodyMarkdown:  Hi have a page which has List data.   When I click on an element of the list, an Ajax function is called which populates some date in the right column.  Now wat's working:-  i click on element -> i see the loading.gif a few seconds -> data gets loaded.  When i click on other element midway, the first function completes and only then the second request is taken.  What I want:-  when i click and the ajax is loading, I click again in between, the previous function should stop, and my new request should be taken. \n",
      "Enter the Tags (space separated):  jquery ajax\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔮 Predicted Label: 1 (Open)\n",
      "📊 Probabilities: [0.3762441873550415, 0.6237558722496033]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# Suppress warnings\n",
    "hf_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Load fine-tuned weights\n",
    "state_dict = torch.load(\"/kaggle/working/best_fine_tuned_bert.pth\", map_location=torch.device('cpu'))\n",
    "\n",
    "# Fix DataParallel prefixes if any\n",
    "new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()\n",
    "\n",
    "# Function to predict\n",
    "def predict(title, body, tags):\n",
    "    combined_input = f\"{title} [SEP] {body} [SEP] {tags}\"\n",
    "    inputs = tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        prediction = torch.argmax(probs, dim=1).item()\n",
    "    \n",
    "    return prediction, probs.squeeze().tolist()\n",
    "\n",
    "# User input\n",
    "title_input = input(\"Enter the Title: \")\n",
    "body_input = input(\"Enter the BodyMarkdown: \")\n",
    "tags_input = input(\"Enter the Tags (space separated): \")\n",
    "\n",
    "# Run prediction\n",
    "label, probabilities = predict(title_input, body_input, tags_input)\n",
    "\n",
    "# Output\n",
    "print(f\"\\n🔮 Predicted Label: {label} ({'Closed' if label == 0 else 'Open'})\")\n",
    "print(f\"📊 Probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T02:19:55.909270Z",
     "iopub.status.busy": "2025-04-06T02:19:55.908982Z",
     "iopub.status.idle": "2025-04-06T04:03:29.661088Z",
     "shell.execute_reply": "2025-04-06T04:03:29.660118Z",
     "shell.execute_reply.started": "2025-04-06T02:19:55.909248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda with 2 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT embeddings: 100%|██████████| 12000/12000 [12:46<00:00, 15.66it/s]\n",
      "Extracting BERT embeddings: 100%|██████████| 12000/12000 [37:08<00:00,  5.38it/s]\n",
      "Extracting BERT embeddings: 100%|██████████| 12000/12000 [38:04<00:00,  5.25it/s]\n",
      "Extracting BERT embeddings: 100%|██████████| 12000/12000 [12:29<00:00, 16.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (BERT) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.68      0.71      7998\n",
      "           1       0.78      0.83      0.81     11201\n",
      "\n",
      "    accuracy                           0.77     19199\n",
      "   macro avg       0.76      0.75      0.76     19199\n",
      "weighted avg       0.77      0.77      0.77     19199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn import DataParallel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if multiple GPUs are available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"Using device: {device} with {n_gpus} GPUs\")\n",
    "\n",
    "# Load Pretrained BERT Tokenizer and Model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model = DataParallel(bert_model)  # Wrap for multi-GPU\n",
    "bert_model.to(device)\n",
    "bert_model.eval()  # Disable dropout etc.\n",
    "\n",
    "# Clean and prepare data\n",
    "df[['Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5']] = df[['Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5']].fillna('')\n",
    "df['Tags_combined'] = df[['Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5']].apply(lambda x: ' '.join(x), axis=1)\n",
    "df = df[['Title', 'BodyMarkdown', 'CodeSnippets', 'Tags_combined', 'OpenStatus']].dropna()\n",
    "df[\"BiStatus\"] = df[\"OpenStatus\"].map(lambda x: 1 if x == \"open\" else 0)\n",
    "\n",
    "# Function to batch process embeddings\n",
    "def get_bert_embeddings(texts, batch_size=8):\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting BERT embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch.tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        \n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "        embeddings.append(cls_embeddings)\n",
    "\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Batch embedding extraction for each feature\n",
    "title_embs = get_bert_embeddings(df['Title'])\n",
    "body_embs = get_bert_embeddings(df['BodyMarkdown'])\n",
    "code_embs = get_bert_embeddings(df['CodeSnippets'])\n",
    "tags_embs = get_bert_embeddings(df['Tags_combined'])\n",
    "\n",
    "# Concatenate all embeddings\n",
    "X = np.hstack([title_embs, body_embs, code_embs, tags_embs])\n",
    "y = df[\"BiStatus\"].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Logistic Regression (BERT) Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:03:29.662693Z",
     "iopub.status.busy": "2025-04-06T04:03:29.662413Z",
     "iopub.status.idle": "2025-04-06T04:03:30.128289Z",
     "shell.execute_reply": "2025-04-06T04:03:30.127414Z",
     "shell.execute_reply.started": "2025-04-06T04:03:29.662670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples saved to 'bert_lr_misclassified.csv'\n"
     ]
    }
   ],
   "source": [
    "# Identify misclassified indices\n",
    "misclassified_indices = np.where(y_pred_lr != y_test)[0]\n",
    "\n",
    "# Convert test split to DataFrame\n",
    "# First, create a temporary test dataframe using the index from train_test_split\n",
    "X_test_indices = df.iloc[X_train.shape[0]:].index  # assumes order is preserved\n",
    "\n",
    "# Now collect misclassified rows using index from original df\n",
    "misclassified_rows = []\n",
    "\n",
    "for idx in misclassified_indices:\n",
    "    row_idx = X_test_indices[idx]\n",
    "    row = df.loc[row_idx]\n",
    "\n",
    "    misclassified_rows.append({\n",
    "        \"Title\": row[\"Title\"],\n",
    "        \"BodyMarkdown\": row[\"BodyMarkdown\"],\n",
    "        \"CodeSnippets\": row[\"CodeSnippets\"],\n",
    "        \"Tags_combined\": row[\"Tags_combined\"],\n",
    "        \"TrueLabel\": row[\"OpenStatus\"],\n",
    "        \"PredictedLabel\": int(y_pred_lr[idx])\n",
    "    })\n",
    "\n",
    "# Create DataFrame from misclassified rows\n",
    "misclassified_df = pd.DataFrame(misclassified_rows)\n",
    "\n",
    "# Save to CSV\n",
    "misclassified_df.to_csv(\"bert_lr_misclassified_binary.csv\", index=False)\n",
    "print(\"Misclassified samples saved to 'bert_lr_misclassified.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:08:27.349216Z",
     "iopub.status.busy": "2025-04-06T04:08:27.348912Z",
     "iopub.status.idle": "2025-04-06T04:14:41.743484Z",
     "shell.execute_reply": "2025-04-06T04:14:41.742603Z",
     "shell.execute_reply.started": "2025-04-06T04:08:27.349193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training Logistic Regression for multi-class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (Multiclass BERT) Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "               open       0.75      0.88      0.81     11201\n",
      "      too localized       0.28      0.08      0.13      1019\n",
      "          off topic       0.54      0.44      0.48      1920\n",
      "not a real question       0.51      0.41      0.46      3367\n",
      "   not constructive       0.56      0.51      0.53      1692\n",
      "\n",
      "           accuracy                           0.68     19199\n",
      "          macro avg       0.53      0.46      0.48     19199\n",
      "       weighted avg       0.65      0.68      0.65     19199\n",
      "\n",
      "Total misclassified samples: 6194\n",
      "Saved misclassified samples to 'bert_multiclass_misclassified.csv'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "unique_labels = df[\"OpenStatus\"].unique()\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Map labels to numeric form\n",
    "df[\"OpenStatusLabel\"] = df[\"OpenStatus\"].map(label_mapping)\n",
    "\n",
    "\n",
    "X = np.hstack([title_embs, body_embs, code_embs, tags_embs])  # Shape: (num_samples, embedding_size * 4)\n",
    "y = df[\"OpenStatusLabel\"].values\n",
    "\n",
    "# ====== TRAIN-TEST SPLIT ======\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ====== MODEL TRAINING ======\n",
    "print(\"Training Logistic Regression for multi-class...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# ====== EVALUATION ======\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "print(\"Logistic Regression (Multiclass BERT) Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=[reverse_label_mapping[i] for i in sorted(reverse_label_mapping)]))\n",
    "\n",
    "# ====== MISCLASSIFIED SAMPLES ======\n",
    "\n",
    "misclassified_indices = np.where(y_pred_lr != y_test)[0]\n",
    "print(f\"Total misclassified samples: {len(misclassified_indices)}\")\n",
    "\n",
    "# Find indices of X_test in original df (preserving row alignment)\n",
    "X_test_indices = df.iloc[X_train.shape[0]:].index\n",
    "\n",
    "misclassified_rows = []\n",
    "for idx in misclassified_indices:\n",
    "    row_idx = X_test_indices[idx]\n",
    "    row = df.loc[row_idx]\n",
    "    misclassified_rows.append({\n",
    "        \"Title\": row[\"Title\"],\n",
    "        \"BodyMarkdown\": row[\"BodyMarkdown\"],\n",
    "        \"CodeSnippets\": row[\"CodeSnippets\"],\n",
    "        \"Tags_combined\": row[\"Tags_combined\"],\n",
    "        \"TrueLabel\": reverse_label_mapping[row[\"OpenStatusLabel\"]],\n",
    "        \"PredictedLabel\": reverse_label_mapping[y_pred_lr[idx]]\n",
    "    })\n",
    "\n",
    "misclassified_df = pd.DataFrame(misclassified_rows)\n",
    "misclassified_df.to_csv(\"bertpretained_multiclass_misclassified.csv\", index=False)\n",
    "print(\"Saved misclassified samples to 'bert_multiclass_misclassified.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6902420,
     "sourceId": 11075338,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
